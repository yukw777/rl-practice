{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01329987,  0.02098095,  0.00331455, -0.0332566 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two possible actions, 0 and 1\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01288026  0.21605521  0.00264941 -0.32489191]\n",
      "1.0\n",
      "False\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "action = 1 # accelerate to the right\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.048000000000002, 9.1449273370541331, 25.0, 68.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPoleNet (\n",
      "  (hidden): Linear (4 -> 4)\n",
      "  (logits): Linear (4 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CartPoleNet(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super(CartPoleNet, self).__init__()\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.logits = nn.Linear(n_hidden, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.hidden(x))\n",
    "        # output is the probability of going left\n",
    "        return F.sigmoid(self.logits(x))\n",
    "\n",
    "net = CartPoleNet(4, 4, 1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5551\n",
      " 0.7948\n",
      "-0.0617\n",
      "-0.0369\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "Variable containing:\n",
      " 0.5250\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.4750\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5250\n",
      " 0.4750\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(4))\n",
    "input = Variable(torch.randn(4))\n",
    "output = net(input)\n",
    "print(output)\n",
    "print(1 - output)\n",
    "print(torch.cat((output, 1 - output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_action(prob):\n",
    "    # we're not really using pytorch's stochastic reinforcement mechanism here\n",
    "    # since we're calculating the gradients directly\n",
    "    # so we need to use numpy's multinomial so as not to make this a stochastic tensor\n",
    "    # https://discuss.pytorch.org/t/backpropagate-on-a-stochastic-variable/3496/13\n",
    "    raw_prob = prob.data[0]\n",
    "    # actions has the number of times each action was chosen.\n",
    "    actions = np.random.multinomial(1, (raw_prob, 1 - raw_prob))\n",
    "    # so we need to drop the first element since that's the number of times \"left\" was selected\n",
    "    # if left was selected, the first element would be 1, and the second element would be 0\n",
    "    # since action 0 is left, we want to keep the second element.\n",
    "    action = np.delete(actions, 0)\n",
    "    return torch.from_numpy(action)\n",
    "    #return torch.multinomial(torch.cat((prob, 1 - prob)), 1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(get_action(net(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0060\n",
      "-0.1989\n",
      " 0.0269\n",
      " 0.2569\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4654\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, award, done, info = env.step(get_action(net(input))[0])\n",
    "tensor = torch.from_numpy(obs).float()\n",
    "print(tensor)\n",
    "net(Variable(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = CartPoleNet(4, 4, 1)\n",
    "action = get_action(net(input))\n",
    "print(action)\n",
    "\n",
    "# if action is 0 (left), the target probability must be 1 since it's the probability of going left, vice versa\n",
    "y = 1. - action.float()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount and normalize rewards\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    #all_normalized_rewards = \n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n",
    "    #for action, reward in zip(all_actions, all_normalized_rewards):\n",
    "    #    action.reinforce(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast step:    13\t\n",
      "Episode 10\tLast step:    25\t\n",
      "Episode 20\tLast step:    32\t\n",
      "Episode 30\tLast step:    19\t\n",
      "Episode 40\tLast step:    23\t\n",
      "Episode 50\tLast step:    20\t\n",
      "Episode 60\tLast step:    26\t\n",
      "Episode 70\tLast step:    12\t\n",
      "Episode 80\tLast step:    15\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-66a83b4180a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_max_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# probability to go left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mcurrent_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ad7b3c97bff1>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(prob)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mraw_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# actions has the number of times each action was chosen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mraw_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mraw_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# so we need to drop the first element since that's the number of times \"left\" was selected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# if left was selected, the first element would be 1, and the second element would be 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "net = CartPoleNet(4, 4, 1)\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# loss criterion\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "# optimizer\n",
    "# the book uses lr = 0.01, but let's see if this works well\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "n_iter = 250  # number of training iterations\n",
    "n_max_steps = 1000 # max steps per episode\n",
    "n_games_per_update = 10 # train the policy every 10 episodes\n",
    "discount_rate = 0.95\n",
    "\n",
    "for iteration in range(n_iter):\n",
    "    all_rewards = []  # all sequences of raw rewards for each episode\n",
    "    all_gradients = []   # gradients saved at each step of each episode\n",
    "    for game in range(n_games_per_update):\n",
    "        current_rewards = []  # all raw rewards from the current episode\n",
    "        current_gradients = [] # all gradients from the current episode\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            out = net(Variable(torch.from_numpy(obs).float()))  # probability to go left\n",
    "            action = get_action(out)\n",
    "            obs, reward, done, info = env.step(action[0])\n",
    "            current_rewards.append(reward)\n",
    "            y = Variable(1. - action.float())\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            current_gradients.append([p.grad.clone().data.numpy() for p in net.parameters()])\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_gradients.append(current_gradients)\n",
    "    # at this point, we have run the policy for 10 episodes, and we're ready for a \n",
    "    # policy update.\n",
    "    optimizer.zero_grad()\n",
    "    all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "    for par_index, param in enumerate(net.parameters()):\n",
    "        mean_gradients = np.mean(\n",
    "            [reward * all_gradients[game_index][step][par_index]\n",
    "                for game_index, rewards in enumerate(all_rewards)\n",
    "                for step, reward in enumerate(rewards)],\n",
    "            axis=0\n",
    "        )\n",
    "        param.grad = Variable(torch.from_numpy(mean_gradients))\n",
    "    optimizer.step()\n",
    "    if iteration % 10 == 0:\n",
    "        print('Episode {}\\tLast step: {:5d}\\t'.format(\n",
    "            iteration, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.994, 2.4927823811957595, 8.0, 27.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "def evaluate(model):\n",
    "    totals = []\n",
    "    for episode in range(500):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for step in range(1000):\n",
    "            out = model(Variable(torch.from_numpy(obs).float()))\n",
    "            action = get_action(out)\n",
    "            obs, reward, done, info = env.step(action[0])\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        totals.append(episode_rewards)\n",
    "    return np.mean(totals), np.std(totals), np.min(totals), np.max(totals)\n",
    "\n",
    "evaluate(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast length:     9\tAverage length: 10.41\n",
      "Episode 20\tLast length:    10\tAverage length: 10.28\n",
      "Episode 30\tLast length:     8\tAverage length: 10.19\n",
      "Episode 40\tLast length:     8\tAverage length: 10.26\n",
      "Episode 50\tLast length:    33\tAverage length: 11.15\n",
      "Episode 60\tLast length:    23\tAverage length: 13.64\n",
      "Episode 70\tLast length:    15\tAverage length: 13.99\n",
      "Episode 80\tLast length:    57\tAverage length: 15.49\n",
      "Episode 90\tLast length:    37\tAverage length: 19.78\n",
      "Episode 100\tLast length:    11\tAverage length: 20.64\n",
      "Episode 110\tLast length:    22\tAverage length: 20.70\n",
      "Episode 120\tLast length:    24\tAverage length: 21.44\n",
      "Episode 130\tLast length:    55\tAverage length: 22.45\n",
      "Episode 140\tLast length:    56\tAverage length: 24.52\n",
      "Episode 150\tLast length:   100\tAverage length: 28.33\n",
      "Episode 160\tLast length:   251\tAverage length: 42.49\n",
      "Episode 170\tLast length:   293\tAverage length: 80.82\n",
      "Solved! Running reward is now 100.02474666886519 and the last episode runs to 499 time steps!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "#n_iter = 250  # number of training iterations\n",
    "discount_rate = 0.95\n",
    "n_max_steps = 1000 # max steps per episode\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores)\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    action = probs.multinomial()\n",
    "    policy.saved_actions.append(action)\n",
    "    return action.data\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + discount_rate * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for action, r in zip(policy.saved_actions, rewards):\n",
    "        action.reinforce(r)\n",
    "    optimizer.zero_grad()\n",
    "    autograd.backward(policy.saved_actions, [None for _ in policy.saved_actions])\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_actions[:]\n",
    "\n",
    "\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(n_max_steps): # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action[0,0])\n",
    "        #env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    if i_episode % 10 == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > 100:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479.04199999999997, 47.767062250048411, 215.0, 500.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "def evaluate(model):\n",
    "    totals = []\n",
    "    for episode in range(500):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for step in range(1000):\n",
    "            obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "            probs = model(Variable(obs))\n",
    "            action = probs.multinomial()\n",
    "            obs, reward, done, _ = env.step(action.data[0,0])\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        totals.append(episode_rewards)\n",
    "    return np.mean(totals), np.std(totals), np.min(totals), np.max(totals)\n",
    "\n",
    "evaluate(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
