{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01329987,  0.02098095,  0.00331455, -0.0332566 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two possible actions, 0 and 1\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01288026  0.21605521  0.00264941 -0.32489191]\n",
      "1.0\n",
      "False\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "action = 1 # accelerate to the right\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.048000000000002, 9.1449273370541331, 25.0, 68.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPoleNet (\n",
      "  (hidden): Linear (4 -> 4)\n",
      "  (logits): Linear (4 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CartPoleNet(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super(CartPoleNet, self).__init__()\n",
    "        self.hidden = nn.Linear(n_inputs, n_hidden)\n",
    "        self.logits = nn.Linear(n_hidden, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.hidden(x))\n",
    "        # output is the probability of going left\n",
    "        return F.sigmoid(self.logits(x))\n",
    "\n",
    "net = CartPoleNet(4, 4, 1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5551\n",
      " 0.7948\n",
      "-0.0617\n",
      "-0.0369\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "Variable containing:\n",
      " 0.5250\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.4750\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5250\n",
      " 0.4750\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(4))\n",
    "input = Variable(torch.randn(4))\n",
    "output = net(input)\n",
    "print(output)\n",
    "print(1 - output)\n",
    "print(torch.cat((output, 1 - output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_action(prob):\n",
    "    # we're not really using pytorch's stochastic reinforcement mechanism here\n",
    "    # since we're calculating the gradients directly\n",
    "    # so we need to use numpy's multinomial so as not to make this a stochastic tensor\n",
    "    # https://discuss.pytorch.org/t/backpropagate-on-a-stochastic-variable/3496/13\n",
    "    raw_prob = prob.data[0]\n",
    "    # actions has the number of times each action was chosen.\n",
    "    actions = np.random.multinomial(1, (raw_prob, 1 - raw_prob))\n",
    "    # so we need to drop the first element since that's the number of times \"left\" was selected\n",
    "    # if left was selected, the first element would be 1, and the second element would be 0\n",
    "    # since action 0 is left, we want to keep the second element.\n",
    "    action = np.delete(actions, 0)\n",
    "    return torch.from_numpy(action)\n",
    "    #return torch.multinomial(torch.cat((prob, 1 - prob)), 1)\n",
    "\n",
    "for i in range(10):\n",
    "    print(get_action(net(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0060\n",
      "-0.1989\n",
      " 0.0269\n",
      " 0.2569\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4654\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "obs, award, done, info = env.step(get_action(net(input))[0])\n",
    "tensor = torch.from_numpy(obs).float()\n",
    "print(tensor)\n",
    "net(Variable(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = CartPoleNet(4, 4, 1)\n",
    "action = get_action(net(input))\n",
    "print(action)\n",
    "\n",
    "# if action is 0 (left), the target probability must be 1 since it's the probability of going left, vice versa\n",
    "y = 1. - action.float()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount and normalize rewards\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    #all_normalized_rewards = \n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n",
    "    #for action, reward in zip(all_actions, all_normalized_rewards):\n",
    "    #    action.reinforce(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast step:    28\t\n",
      "Episode 10\tLast step:    21\t\n",
      "Episode 20\tLast step:    31\t\n",
      "Episode 30\tLast step:    37\t\n",
      "Episode 40\tLast step:    12\t\n",
      "Episode 50\tLast step:    18\t\n",
      "Episode 60\tLast step:    14\t\n",
      "Episode 70\tLast step:    13\t\n",
      "Episode 80\tLast step:    17\t\n",
      "Episode 90\tLast step:    18\t\n",
      "Episode 100\tLast step:    28\t\n",
      "Episode 110\tLast step:    10\t\n",
      "Episode 120\tLast step:    17\t\n",
      "Episode 130\tLast step:    11\t\n",
      "Episode 140\tLast step:     7\t\n",
      "Episode 150\tLast step:    11\t\n",
      "Episode 160\tLast step:     9\t\n",
      "Episode 170\tLast step:     9\t\n",
      "Episode 180\tLast step:    11\t\n",
      "Episode 190\tLast step:    21\t\n",
      "Episode 200\tLast step:    14\t\n",
      "Episode 210\tLast step:    10\t\n",
      "Episode 220\tLast step:    16\t\n",
      "Episode 230\tLast step:    20\t\n",
      "Episode 240\tLast step:     9\t\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "net = CartPoleNet(4, 4, 1)\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "# the book uses lr = 0.01, but let's see if this works well\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "n_iter = 250  # number of training iterations\n",
    "n_max_steps = 1000 # max steps per episode\n",
    "n_games_per_update = 10 # train the policy every 10 episodes\n",
    "discount_rate = 0.95\n",
    "\n",
    "for iteration in range(n_iter):\n",
    "    all_rewards = []  # all sequences of raw rewards for each episode\n",
    "    all_gradients = []   # gradients saved at each step of each episode\n",
    "    for game in range(n_games_per_update):\n",
    "        current_rewards = []  # all raw rewards from the current episode\n",
    "        current_gradients = [] # all gradients from the current episode\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            out = net(Variable(torch.from_numpy(obs).float()))  # probability to go left\n",
    "            action = get_action(out)\n",
    "            obs, reward, done, info = env.step(action[0])\n",
    "            current_rewards.append(reward)\n",
    "            y = Variable(1. - action.float())\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            current_gradients.append([p.grad.clone().data.numpy() for p in net.parameters()])\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_gradients.append(current_gradients)\n",
    "    # at this point, we have run the policy for 10 episodes, and we're ready for a \n",
    "    # policy update.\n",
    "    optimizer.zero_grad()\n",
    "    all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "    for par_index, param in enumerate(net.parameters()):\n",
    "        mean_gradients = np.mean(\n",
    "            [reward * all_gradients[game_index][step][par_index]\n",
    "                for game_index, rewards in enumerate(all_rewards)\n",
    "                for step, reward in enumerate(rewards)],\n",
    "            axis=0\n",
    "        )\n",
    "        param.grad = Variable(torch.from_numpy(mean_gradients))\n",
    "    optimizer.step()\n",
    "    if iteration % 10 == 0:\n",
    "        print('Episode {}\\tLast step: {:5d}\\t'.format(\n",
    "            iteration, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.994, 2.4927823811957595, 8.0, 27.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "def evaluate(model):\n",
    "    totals = []\n",
    "    for episode in range(500):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for step in range(1000):\n",
    "            out = model(Variable(torch.from_numpy(obs).float()))\n",
    "            action = get_action(out)\n",
    "            obs, reward, done, info = env.step(action[0])\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        totals.append(episode_rewards)\n",
    "    return np.mean(totals), np.std(totals), np.min(totals), np.max(totals)\n",
    "\n",
    "evaluate(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast length:    22\tAverage length: 12.20\n",
      "Episode 20\tLast length:    69\tAverage length: 16.16\n",
      "Episode 30\tLast length:    93\tAverage length: 24.72\n",
      "Episode 40\tLast length:    94\tAverage length: 32.71\n",
      "Episode 50\tLast length:   165\tAverage length: 50.06\n",
      "Episode 60\tLast length:    55\tAverage length: 51.92\n",
      "Episode 70\tLast length:   306\tAverage length: 59.58\n",
      "Episode 80\tLast length:   126\tAverage length: 69.65\n",
      "Episode 90\tLast length:    48\tAverage length: 73.95\n",
      "Episode 100\tLast length:   160\tAverage length: 78.44\n",
      "Episode 110\tLast length:   164\tAverage length: 85.45\n",
      "Episode 120\tLast length:   158\tAverage length: 91.52\n",
      "Episode 130\tLast length:   124\tAverage length: 94.66\n",
      "Episode 140\tLast length:   106\tAverage length: 96.83\n",
      "Episode 150\tLast length:    83\tAverage length: 92.48\n",
      "Episode 160\tLast length:   125\tAverage length: 93.00\n",
      "Episode 170\tLast length:   132\tAverage length: 99.80\n",
      "Episode 180\tLast length:   128\tAverage length: 104.77\n",
      "Episode 190\tLast length:   290\tAverage length: 112.88\n",
      "Episode 200\tLast length:   279\tAverage length: 130.91\n",
      "Episode 210\tLast length:   305\tAverage length: 142.58\n",
      "Episode 220\tLast length:   125\tAverage length: 145.42\n",
      "Episode 230\tLast length:   131\tAverage length: 144.97\n",
      "Episode 240\tLast length:   159\tAverage length: 144.99\n",
      "Episode 250\tLast length:   112\tAverage length: 143.13\n",
      "Episode 260\tLast length:   172\tAverage length: 142.97\n",
      "Episode 270\tLast length:   209\tAverage length: 146.23\n",
      "Episode 280\tLast length:   281\tAverage length: 155.88\n",
      "Episode 290\tLast length:   442\tAverage length: 175.42\n",
      "Episode 300\tLast length:   213\tAverage length: 192.96\n",
      "Episode 310\tLast length:   140\tAverage length: 188.95\n",
      "Episode 320\tLast length:   110\tAverage length: 180.83\n",
      "Episode 330\tLast length:   114\tAverage length: 173.17\n",
      "Episode 340\tLast length:    10\tAverage length: 165.89\n",
      "Episode 350\tLast length:   120\tAverage length: 156.57\n",
      "Episode 360\tLast length:   133\tAverage length: 151.89\n",
      "Episode 370\tLast length:   172\tAverage length: 151.53\n",
      "Episode 380\tLast length:   168\tAverage length: 152.70\n",
      "Episode 390\tLast length:   287\tAverage length: 159.26\n",
      "Episode 400\tLast length:   499\tAverage length: 180.98\n",
      "Solved! Running reward is now 201.98992138564452 and the last episode runs to 489 time steps!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "#n_iter = 250  # number of training iterations\n",
    "discount_rate = 0.95\n",
    "n_max_steps = 1000 # max steps per episode\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores)\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(Variable(state))\n",
    "    action = probs.multinomial()\n",
    "    policy.saved_actions.append(action)\n",
    "    return action.data\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + discount_rate * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for action, r in zip(policy.saved_actions, rewards):\n",
    "        action.reinforce(r)\n",
    "    optimizer.zero_grad()\n",
    "    autograd.backward(policy.saved_actions, [None for _ in policy.saved_actions])\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_actions[:]\n",
    "\n",
    "\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(n_max_steps): # Don't infinite loop while learning\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action[0,0])\n",
    "        #env.render()\n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    if i_episode % 10 == 0:\n",
    "        print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "            i_episode, t, running_reward))\n",
    "    if running_reward > 200:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498.05200000000002, 6.0687145261579074, 454.0, 500.0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "def evaluate(model):\n",
    "    totals = []\n",
    "    for episode in range(500):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for step in range(1000):\n",
    "            obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "            probs = model(Variable(obs))\n",
    "            action = probs.multinomial()\n",
    "            obs, reward, done, _ = env.step(action.data[0,0])\n",
    "            episode_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        totals.append(episode_rewards)\n",
    "    return np.mean(totals), np.std(totals), np.min(totals), np.max(totals)\n",
    "\n",
    "evaluate(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
